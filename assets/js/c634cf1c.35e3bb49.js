"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[9058],{1867:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>h,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"chapter-5-humanoid/vision-language-action","title":"Vision-Language-Action (VLA): The Cognitive Core \ud83e\udde0\ud83d\udc41\ufe0f\u270b","description":"From Chatbots to Robobots","source":"@site/docs/chapter-5-humanoid/vision-language-action.md","sourceDirName":"chapter-5-humanoid","slug":"/chapter-5-humanoid/vision-language-action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-5-humanoid/vision-language-action","draft":false,"unlisted":false,"editUrl":"https://github.com/mub7865/Physical-AI-Humanoid-Robotics-Textbook/tree/main/book/docs/chapter-5-humanoid/vision-language-action.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid VLA Development","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-5-humanoid/"},"next":{"title":"Weekly Breakdown: Weeks 11-12 \u2013 Building the Humanoid \ud83d\uddd3\ufe0f","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-5-humanoid/weekly-breakdown-weeks-11-12"}}');var i=t(4848),s=t(8453);const r={},a="Vision-Language-Action (VLA): The Cognitive Core \ud83e\udde0\ud83d\udc41\ufe0f\u270b",h={},c=[{value:"From Chatbots to Robobots",id:"from-chatbots-to-robobots",level:2},{value:"1. The Ears: Voice-to-Action with Whisper \ud83d\udc42",id:"1-the-ears-voice-to-action-with-whisper-",level:2},{value:"How it Works:",id:"how-it-works",level:3},{value:"Code Example: Python Listener \ud83d\udc0d",id:"code-example-python-listener-",level:3}];function l(e){const o={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.header,{children:(0,i.jsx)(o.h1,{id:"vision-language-action-vla-the-cognitive-core-\ufe0f",children:"Vision-Language-Action (VLA): The Cognitive Core \ud83e\udde0\ud83d\udc41\ufe0f\u270b"})}),"\n",(0,i.jsx)(o.h2,{id:"from-chatbots-to-robobots",children:"From Chatbots to Robobots"}),"\n",(0,i.jsxs)(o.p,{children:["You have likely used ChatGPT. You ask it a question, and it gives you text back.\r\nBut what if you asked ChatGPT to ",(0,i.jsx)(o.em,{children:'"Make me a sandwich"'}),"?\r\nIn the digital world, it can only give you a ",(0,i.jsx)(o.em,{children:"recipe"}),". It cannot physically go to the kitchen, find the bread, and spread the jam."]}),"\n",(0,i.jsxs)(o.p,{children:[(0,i.jsx)(o.strong,{children:"Vision-Language-Action (VLA)"})," models bridge this gap. They take ",(0,i.jsx)(o.strong,{children:"Vision"})," (what the robot sees) and ",(0,i.jsx)(o.strong,{children:"Language"})," (what you say) and convert them into ",(0,i.jsx)(o.strong,{children:"Action"})," (motor movements)."]}),"\n",(0,i.jsx)(o.p,{children:"This is the technology that powers robots like Google's RT-2 and Tesla's Optimus. In this chapter, we will build a simplified VLA pipeline using OpenAI's tools."}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsx)(o.h2,{id:"1-the-ears-voice-to-action-with-whisper-",children:"1. The Ears: Voice-to-Action with Whisper \ud83d\udc42"}),"\n",(0,i.jsxs)(o.p,{children:["The first step in natural interaction is ",(0,i.jsx)(o.strong,{children:"Hearing"}),". We don't want to type commands on a keyboard; we want to speak to our robot."]}),"\n",(0,i.jsxs)(o.p,{children:["We use ",(0,i.jsx)(o.strong,{children:"OpenAI Whisper"}),", a state-of-the-art automatic speech recognition (ASR) system."]}),"\n",(0,i.jsx)(o.h3,{id:"how-it-works",children:"How it Works:"}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.strong,{children:"Audio Capture:"})," The robot's microphone records a waveform."]}),"\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.strong,{children:"Transcription:"})," Whisper converts that waveform into text: ",(0,i.jsx)(o.em,{children:'"Robot, please pick up the blue bottle."'})]}),"\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.strong,{children:"Forwarding:"})," This text is sent to the LLM Brain."]}),"\n"]}),"\n",(0,i.jsx)(o.h3,{id:"code-example-python-listener-",children:"Code Example: Python Listener \ud83d\udc0d"}),"\n",(0,i.jsx)(o.p,{children:'Here is how you implement a simple "Ear" for your robot using Python:'}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:'import openai\r\n\r\ndef listen_to_command(audio_file_path):\r\n    audio_file = open(audio_file_path, "rb")\r\n    transcript = openai.Audio.transcribe("whisper-1", audio_file)\r\n    return transcript["text"]\r\n\r\n# Usage\r\ncommand = listen_to_command("command.mp3")\r\nprint(f"I heard: {command}")\n'})})]})}function d(e={}){const{wrapper:o}={...(0,s.R)(),...e.components};return o?(0,i.jsx)(o,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},8453:(e,o,t)=>{t.d(o,{R:()=>r,x:()=>a});var n=t(6540);const i={},s=n.createContext(i);function r(e){const o=n.useContext(s);return n.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function a(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),n.createElement(s.Provider,{value:o},e.children)}}}]);