"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[7048],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},9626:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-5-humanoid/index","title":"Chapter 5: Humanoid Robot Development & VLA \ud83e\udd16\ud83d\udde3\ufe0f","description":"The Final Frontier: Embodied Intelligence","source":"@site/docs/chapter-5-humanoid/index.md","sourceDirName":"chapter-5-humanoid","slug":"/chapter-5-humanoid/","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-5-humanoid/","draft":false,"unlisted":false,"editUrl":"https://github.com/mub7865/Physical-AI-Humanoid-Robotics-Textbook/tree/main/book/docs/chapter-5-humanoid/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Weekly Breakdown: Weeks 8-10 \u2013 The AI-Robot Brain \ud83d\uddd3\ufe0f","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-4-isaac/weekly-breakdown-weeks-8-10"},"next":{"title":"Vision-Language-Action (VLA): The Cognitive Core \ud83e\udde0\ud83d\udc41\ufe0f\u270b","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/chapter-5-humanoid/vision-language-action-(vla)"}}');var o=t(4848),s=t(8453);const a={},l="Chapter 5: Humanoid Robot Development & VLA \ud83e\udd16\ud83d\udde3\ufe0f",r={},c=[{value:"The Final Frontier: Embodied Intelligence",id:"the-final-frontier-embodied-intelligence",level:2},{value:"[cite_start]1. What is VLA (Vision-Language-Action)? [cite: 68]",id:"cite_start1-what-is-vla-vision-language-action-cite-68",level:2},{value:"The Pipeline:",id:"the-pipeline",level:3},{value:"[cite_start]2. Humanoid Specifics: Walking on Two Legs [cite: 104]",id:"cite_start2-humanoid-specifics-walking-on-two-legs-cite-104",level:2},{value:"[cite_start]A. Kinematics &amp; Dynamics [cite: 105]",id:"cite_starta-kinematics--dynamics-cite-105",level:3},{value:"[cite_start]B. Bipedal Locomotion &amp; Balance [cite: 106]",id:"cite_startb-bipedal-locomotion--balance-cite-106",level:3},{value:"[cite_start]C. Manipulation [cite: 107]",id:"cite_startc-manipulation-cite-107",level:3},{value:"[cite_start]3. Cognitive Planning: LLMs as Planners [cite: 71]",id:"cite_start3-cognitive-planning-llms-as-planners-cite-71",level:2},{value:"[cite_start]4. The Capstone Project: The Autonomous Humanoid [cite: 72]",id:"cite_start4-the-capstone-project-the-autonomous-humanoid-cite-72",level:2},{value:"Summary of Module 4 Roadmap",id:"summary-of-module-4-roadmap",level:3},{value:"Ready to Talk to Your Robot?",id:"ready-to-talk-to-your-robot",level:3}];function d(e){const n={blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-5-humanoid-robot-development--vla-\ufe0f",children:"Chapter 5: Humanoid Robot Development & VLA \ud83e\udd16\ud83d\udde3\ufe0f"})}),"\n",(0,o.jsx)(n.h2,{id:"the-final-frontier-embodied-intelligence",children:"The Final Frontier: Embodied Intelligence"}),"\n",(0,o.jsxs)(n.p,{children:["We have built the nervous system (ROS 2), simulated the world (Gazebo), and given the robot sight (Isaac Sim). Now, we give it a ",(0,o.jsx)(n.strong,{children:"Voice"})," and a ",(0,o.jsx)(n.strong,{children:"Reasoning Mind"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["This chapter introduces ",(0,o.jsx)(n.strong,{children:"Module 4: Vision-Language-Action (VLA)"}),". It represents the convergence of Large Language Models (like GPT-4) with Robotics."]}),"\n",(0,o.jsx)(n.p,{children:'Until recently, robots were "dumb." You had to code every single movement.'}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Old Way:"})," ",(0,o.jsx)(n.code,{children:"robot.move_to(x=10, y=20)"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"New Way:"}),' "Robot, go find the red cup in the kitchen."']}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This shift from explicit coding to ",(0,o.jsx)(n.strong,{children:"Semantic Understanding"})," is what defines modern Physical AI."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"cite_start1-what-is-vla-vision-language-action-cite-68",children:"[cite_start]1. What is VLA (Vision-Language-Action)? [cite: 68]"}),"\n",(0,o.jsxs)(n.p,{children:["VLA models are AI systems that can understand ",(0,o.jsx)(n.strong,{children:"Images (Vision)"})," and ",(0,o.jsx)(n.strong,{children:"Text (Language)"})," and translate them into ",(0,o.jsx)(n.strong,{children:"Physical Movements (Action)"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["Imagine showing a robot a picture of a messy room and saying, ",(0,o.jsx)(n.em,{children:'"Clean this up."'}),"\r\nA VLA model does the following:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision:"})," Identifies objects (socks, toys, trash)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language:"}),' Understands the command "Clean up" means "Put objects in their containers."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action:"})," Generates the specific motor commands to pick up the sock and drop it in the basket."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"the-pipeline",children:"The Pipeline:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["[cite_start]",(0,o.jsx)(n.strong,{children:"Ears:"})," OpenAI Whisper (Speech-to-Text)[cite: 70]."]}),"\n",(0,o.jsxs)(n.li,{children:["[cite_start]",(0,o.jsx)(n.strong,{children:"Brain:"})," LLM (Cognitive Planning)[cite: 71]."]}),"\n",(0,o.jsxs)(n.li,{children:["[cite_start]",(0,o.jsx)(n.strong,{children:"Body:"})," ROS 2 (Execution)[cite: 53]."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"cite_start2-humanoid-specifics-walking-on-two-legs-cite-104",children:"[cite_start]2. Humanoid Specifics: Walking on Two Legs [cite: 104]"}),"\n",(0,o.jsxs)(n.p,{children:["Humanoid robots are unique because they are ",(0,o.jsx)(n.strong,{children:"Bipedal"})," (two-legged). This makes them incredibly versatile (they can climb stairs) but mechanially unstable (they fall over easily)."]}),"\n",(0,o.jsx)(n.p,{children:"In this module, we will explore the physics of humanoid movement:"}),"\n",(0,o.jsx)(n.h3,{id:"cite_starta-kinematics--dynamics-cite-105",children:"[cite_start]A. Kinematics & Dynamics [cite: 105]"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Kinematics:"})," Calculating the angles of joints (knees, hips) needed to place the foot in a specific spot."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dynamics:"})," Calculating the forces and torques needed to move those joints while fighting gravity."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"cite_startb-bipedal-locomotion--balance-cite-106",children:"[cite_start]B. Bipedal Locomotion & Balance [cite: 106]"}),"\n",(0,o.jsxs)(n.p,{children:['Walking is essentially "controlled falling." The robot must constantly shift its ',(0,o.jsx)(n.strong,{children:"Center of Mass (CoM)"})," to stay balanced on one foot while the other swings forward. We use control algorithms (like MPC - Model Predictive Control) to keep the robot upright."]}),"\n",(0,o.jsx)(n.h3,{id:"cite_startc-manipulation-cite-107",children:"[cite_start]C. Manipulation [cite: 107]"}),"\n",(0,o.jsx)(n.p,{children:"Humanoids have hands, not just grippers. We will study how to control multi-fingered hands to grasp delicate objects (like an egg) or use tools (like a drill)."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"cite_start3-cognitive-planning-llms-as-planners-cite-71",children:"[cite_start]3. Cognitive Planning: LLMs as Planners [cite: 71]"}),"\n",(0,o.jsxs)(n.p,{children:["How does a robot know ",(0,o.jsx)(n.em,{children:"how"})," to clean a room? We use LLMs as ",(0,o.jsx)(n.strong,{children:"High-Level Planners"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"The Prompt:"})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:'"I am a robot in a kitchen. I see a sponge, a plate, and a faucet. My goal is to clean the plate. What steps should I take?"'}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"The LLM Output:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Move hand to sponge."}),"\n",(0,o.jsx)(n.li,{children:"Grasp sponge."}),"\n",(0,o.jsx)(n.li,{children:"Move sponge to faucet."}),"\n",(0,o.jsx)(n.li,{children:"Turn on water..."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"We interpret these text steps and convert them into ROS 2 actions."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"cite_start4-the-capstone-project-the-autonomous-humanoid-cite-72",children:"[cite_start]4. The Capstone Project: The Autonomous Humanoid [cite: 72]"}),"\n",(0,o.jsx)(n.p,{children:"Everything you have learned leads to this. In the final weeks, you will build the Capstone Project."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"The Mission:"}),"\r\nYou will deploy a simulated humanoid robot in a house environment."]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Command:"})," You will speak to it: ",(0,o.jsx)(n.em,{children:'"Bring me the blue bottle."'})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Plan:"})," The robot will use an LLM to decompose this task."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigate:"})," It will use Nav2 to walk to the kitchen."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perceive:"})," It will use Computer Vision to find the blue bottle."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Act:"})," It will grasp the bottle and bring it back."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"summary-of-module-4-roadmap",children:"Summary of Module 4 Roadmap"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Section 1:"})," Vision-Language-Action models and Voice control."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Section 2:"})," Humanoid kinematics and balance control."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Section 3:"})," The Capstone Project integration."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"ready-to-talk-to-your-robot",children:"Ready to Talk to Your Robot?"}),"\n",(0,o.jsx)(n.p,{children:"Let's start by understanding how LLMs can control motors."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);